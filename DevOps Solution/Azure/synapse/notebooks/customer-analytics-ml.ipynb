{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9b0b2c-8b94-4c87-8c1a-2f8e3d4b5c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Customer Analytics & Machine Learning\n",
    "\n",
    "## Azure Synapse Analytics - Advanced Customer Intelligence\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Customer segmentation using machine learning\n",
    "- Churn prediction modeling\n",
    "- Customer lifetime value estimation\n",
    "- Real-time recommendation engine\n",
    "\n",
    "**Data Sources**: Azure Data Lake, Azure SQL Database\n",
    "**Compute**: Synapse Spark Pools\n",
    "**ML Framework**: Azure ML, MLflow, Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2b3c4d-5e6f-7g8h-9i0j-k1l2m3n4o5p6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure Synapse specific imports\n",
    "from notebookutils import mssparkutils\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.model import Model\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Configuration\n",
    "STORAGE_ACCOUNT = \"your_storage_account\"\n",
    "CONTAINER_NAME = \"analytics\"\n",
    "DATA_LAKE_PATH = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "\n",
    "print(\"üìö Libraries imported successfully\")\n",
    "print(f\"üèóÔ∏è Data Lake Path: {DATA_LAKE_PATH}\")\n",
    "print(f\"‚ú® Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer transaction data from Azure Data Lake\n",
    "customer_transactions = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(f\"{DATA_LAKE_PATH}/gold/customer_transactions\")\n",
    "\n",
    "# Load customer demographics from Azure SQL Database\n",
    "customer_demographics = spark.read \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://your-server.database.windows.net:1433;database=CustomerDB\") \\\n",
    "    .option(\"dbtable\", \"dbo.CustomerDemographics\") \\\n",
    "    .option(\"user\", mssparkutils.credentials.getSecret(\"your-keyvault\", \"sql-username\")) \\\n",
    "    .option(\"password\", mssparkutils.credentials.getSecret(\"your-keyvault\", \"sql-password\")) \\\n",
    "    .load()\n",
    "\n",
    "# Load product catalog\n",
    "product_catalog = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(f\"{DATA_LAKE_PATH}/processed/product_catalog\")\n",
    "\n",
    "print(f\"üìä Data loaded:\")\n",
    "print(f\"   - Customer Transactions: {customer_transactions.count():,} records\")\n",
    "print(f\"   - Customer Demographics: {customer_demographics.count():,} records\")\n",
    "print(f\"   - Product Catalog: {product_catalog.count():,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and quality assessment\n",
    "print(\"üîç Customer Transaction Data Schema:\")\n",
    "customer_transactions.printSchema()\n",
    "\n",
    "print(\"\\nüìà Customer Transaction Summary:\")\n",
    "customer_transactions.select(\n",
    "    count(\"*\").alias(\"total_transactions\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(\"transaction_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"transaction_amount\").alias(\"avg_transaction_amount\"),\n",
    "    min(\"transaction_date\").alias(\"earliest_transaction\"),\n",
    "    max(\"transaction_date\").alias(\"latest_transaction\")\n",
    ").show()\n",
    "\n",
    "# Check for data quality issues\n",
    "print(\"\\nüîé Data Quality Check:\")\n",
    "customer_transactions.select(\n",
    "    sum(when(col(\"customer_id\").isNull(), 1).otherwise(0)).alias(\"null_customer_ids\"),\n",
    "    sum(when(col(\"transaction_amount\") <= 0, 1).otherwise(0)).alias(\"invalid_amounts\"),\n",
    "    sum(when(col(\"transaction_date\").isNull(), 1).otherwise(0)).alias(\"null_dates\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_features(transactions_df, demographics_df, reference_date=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive customer features for machine learning\n",
    "    \"\"\"\n",
    "    if reference_date is None:\n",
    "        reference_date = datetime.now().date()\n",
    "    \n",
    "    # Customer transaction aggregations\n",
    "    customer_metrics = transactions_df.groupBy(\"customer_id\").agg(\n",
    "        count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "        sum(\"transaction_amount\").alias(\"total_spent\"),\n",
    "        avg(\"transaction_amount\").alias(\"avg_transaction_amount\"),\n",
    "        max(\"transaction_date\").alias(\"last_transaction_date\"),\n",
    "        min(\"transaction_date\").alias(\"first_transaction_date\"),\n",
    "        countDistinct(\"product_category\").alias(\"unique_categories\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "        stddev(\"transaction_amount\").alias(\"transaction_amount_std\"),\n",
    "        \n",
    "        # Time-based features\n",
    "        sum(when(dayofweek(col(\"transaction_date\")).isin([1, 7]), \n",
    "                col(\"transaction_amount\")).otherwise(0)).alias(\"weekend_spending\"),\n",
    "        sum(when(hour(col(\"transaction_timestamp\")).between(9, 17), \n",
    "                col(\"transaction_amount\")).otherwise(0)).alias(\"business_hours_spending\"),\n",
    "        \n",
    "        # Category preferences\n",
    "        sum(when(col(\"product_category\") == \"Electronics\", \n",
    "                col(\"transaction_amount\")).otherwise(0)).alias(\"electronics_spending\"),\n",
    "        sum(when(col(\"product_category\") == \"Clothing\", \n",
    "                col(\"transaction_amount\")).otherwise(0)).alias(\"clothing_spending\"),\n",
    "        sum(when(col(\"product_category\") == \"Books\", \n",
    "                col(\"transaction_amount\")).otherwise(0)).alias(\"books_spending\")\n",
    "    )\n",
    "    \n",
    "    # Calculate derived features\n",
    "    customer_features = customer_metrics \\\n",
    "        .withColumn(\"customer_lifetime_days\", \n",
    "                   datediff(col(\"last_transaction_date\"), col(\"first_transaction_date\"))) \\\n",
    "        .withColumn(\"avg_days_between_purchases\",\n",
    "                   col(\"customer_lifetime_days\") / greatest(col(\"total_transactions\") - 1, lit(1))) \\\n",
    "        .withColumn(\"recency_days\", \n",
    "                   datediff(lit(reference_date), col(\"last_transaction_date\"))) \\\n",
    "        .withColumn(\"frequency_score\", \n",
    "                   col(\"total_transactions\") / greatest(col(\"customer_lifetime_days\"), lit(1)) * 365) \\\n",
    "        .withColumn(\"monetary_score\", col(\"total_spent\")) \\\n",
    "        .withColumn(\"weekend_preference\", \n",
    "                   col(\"weekend_spending\") / col(\"total_spent\")) \\\n",
    "        .withColumn(\"category_diversity\", \n",
    "                   col(\"unique_categories\") / lit(10.0))  # Assuming 10 total categories\n",
    "    \n",
    "    # Join with demographics\n",
    "    complete_features = customer_features.join(\n",
    "        demographics_df.select(\"customer_id\", \"age\", \"gender\", \"city\", \"country\", \n",
    "                              \"income_bracket\", \"education_level\", \"marital_status\"),\n",
    "        \"customer_id\", \"left\"\n",
    "    )\n",
    "    \n",
    "    # Add age groups and income categories\n",
    "    complete_features = complete_features \\\n",
    "        .withColumn(\"age_group\", \n",
    "                   when(col(\"age\") < 25, \"18-24\")\n",
    "                   .when(col(\"age\") < 35, \"25-34\")\n",
    "                   .when(col(\"age\") < 45, \"35-44\")\n",
    "                   .when(col(\"age\") < 55, \"45-54\")\n",
    "                   .when(col(\"age\") < 65, \"55-64\")\n",
    "                   .otherwise(\"65+\")) \\\n",
    "        .withColumn(\"is_high_value\", \n",
    "                   when(col(\"total_spent\") > 1000, 1).otherwise(0)) \\\n",
    "        .withColumn(\"is_frequent_buyer\", \n",
    "                   when(col(\"total_transactions\") > 10, 1).otherwise(0)) \\\n",
    "        .withColumn(\"churn_risk\", \n",
    "                   when(col(\"recency_days\") > 90, 1).otherwise(0))\n",
    "    \n",
    "    return complete_features\n",
    "\n",
    "# Create customer features\n",
    "customer_features = create_customer_features(customer_transactions, customer_demographics)\n",
    "\n",
    "print(f\"‚ú® Customer features created: {customer_features.count():,} customers\")\n",
    "print(\"\\nüéØ Feature Summary:\")\n",
    "customer_features.select(\n",
    "    avg(\"total_spent\").alias(\"avg_total_spent\"),\n",
    "    avg(\"total_transactions\").alias(\"avg_transactions\"),\n",
    "    avg(\"recency_days\").alias(\"avg_recency_days\"),\n",
    "    sum(\"is_high_value\").alias(\"high_value_customers\"),\n",
    "    sum(\"churn_risk\").alias(\"at_risk_customers\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Segmentation with K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "clustering_features = [\n",
    "    \"total_spent\", \"total_transactions\", \"avg_transaction_amount\", \n",
    "    \"recency_days\", \"frequency_score\", \"category_diversity\",\n",
    "    \"weekend_preference\", \"customer_lifetime_days\"\n",
    "]\n",
    "\n",
    "# Handle missing values and create feature vector\n",
    "clustering_data = customer_features.na.fill({\n",
    "    \"total_spent\": 0,\n",
    "    \"total_transactions\": 0,\n",
    "    \"avg_transaction_amount\": 0,\n",
    "    \"recency_days\": 365,\n",
    "    \"frequency_score\": 0,\n",
    "    \"category_diversity\": 0,\n",
    "    \"weekend_preference\": 0.5,\n",
    "    \"customer_lifetime_days\": 0\n",
    "})\n",
    "\n",
    "# Create ML pipeline for clustering\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=clustering_features,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# K-means clustering with different k values\n",
    "print(\"üî¨ Finding optimal number of clusters...\")\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 8)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "    model = pipeline.fit(clustering_data)\n",
    "    predictions = model.transform(clustering_data)\n",
    "    \n",
    "    # Calculate within-cluster sum of squares\n",
    "    wssse = model.stages[-1].summary.trainingCost\n",
    "    silhouette_scores.append(wssse)\n",
    "    print(f\"k={k}: WSSSE = {wssse:.2f}\")\n",
    "\n",
    "# Use k=5 for customer segmentation (optimal based on business requirements)\n",
    "optimal_k = 5\n",
    "kmeans = KMeans(k=optimal_k, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "clustering_pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "clustering_model = clustering_pipeline.fit(clustering_data)\n",
    "customer_segments = clustering_model.transform(clustering_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Customer segmentation completed with k={optimal_k}\")\n",
    "\n",
    "# Analyze segments\n",
    "print(\"\\nüìä Customer Segment Analysis:\")\n",
    "segment_analysis = customer_segments.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"customer_count\"),\n",
    "    avg(\"total_spent\").alias(\"avg_total_spent\"),\n",
    "    avg(\"total_transactions\").alias(\"avg_transactions\"),\n",
    "    avg(\"recency_days\").alias(\"avg_recency\"),\n",
    "    avg(\"frequency_score\").alias(\"avg_frequency\")\n",
    ").orderBy(\"cluster\")\n",
    "\n",
    "segment_analysis.show()\n",
    "\n",
    "# Label segments based on characteristics\n",
    "customer_segments_labeled = customer_segments \\\n",
    "    .withColumn(\"segment_name\",\n",
    "               when(col(\"cluster\") == 0, \"Champions\")\n",
    "               .when(col(\"cluster\") == 1, \"Loyal Customers\")\n",
    "               .when(col(\"cluster\") == 2, \"Potential Loyalists\")\n",
    "               .when(col(\"cluster\") == 3, \"At Risk\")\n",
    "               .when(col(\"cluster\") == 4, \"Cannot Lose Them\")\n",
    "               .otherwise(\"Others\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for churn prediction\n",
    "churn_features = [\n",
    "    \"total_spent\", \"total_transactions\", \"avg_transaction_amount\",\n",
    "    \"recency_days\", \"frequency_score\", \"category_diversity\",\n",
    "    \"weekend_preference\", \"customer_lifetime_days\", \"age\"\n",
    "]\n",
    "\n",
    "# Prepare categorical features\n",
    "categorical_features = [\"gender\", \"age_group\", \"education_level\", \"marital_status\"]\n",
    "\n",
    "# Create churn dataset with balanced target variable\n",
    "churn_data = customer_segments_labeled.select(\n",
    "    [\"customer_id\", \"churn_risk\"] + churn_features + categorical_features\n",
    ").na.drop()\n",
    "\n",
    "print(f\"üìà Churn prediction dataset: {churn_data.count():,} customers\")\n",
    "print(\"\\n‚öñÔ∏è Churn distribution:\")\n",
    "churn_data.groupBy(\"churn_risk\").count().show()\n",
    "\n",
    "# Create ML pipeline for churn prediction\n",
    "# String indexers for categorical variables\n",
    "string_indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "# One-hot encoders\n",
    "one_hot_encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "# Feature assembler\n",
    "all_features = churn_features + [f\"{col}_encoded\" for col in categorical_features]\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Feature scaler\n",
    "feature_scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"churn_risk\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    numTrees=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "churn_pipeline = Pipeline(stages=(\n",
    "    string_indexers + one_hot_encoders + \n",
    "    [feature_assembler, feature_scaler, rf_classifier]\n",
    "))\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = churn_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"üìö Training set: {train_data.count():,} customers\")\n",
    "print(f\"üß™ Test set: {test_data.count():,} customers\")\n",
    "\n",
    "# Train model with MLflow tracking\n",
    "with mlflow.start_run(run_name=\"churn_prediction_rf\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"num_trees\", 100)\n",
    "    mlflow.log_param(\"features\", \",\".join(all_features))\n",
    "    \n",
    "    # Train model\n",
    "    churn_model = churn_pipeline.fit(train_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = churn_model.transform(train_data)\n",
    "    test_predictions = churn_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"churn_risk\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    train_auc = evaluator.evaluate(train_predictions)\n",
    "    test_auc = evaluator.evaluate(test_predictions)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_auc\", train_auc)\n",
    "    mlflow.log_metric(\"test_auc\", test_auc)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.spark.log_model(churn_model, \"churn_model\")\n",
    "    \n",
    "    print(f\"\\nüéØ Model Performance:\")\n",
    "    print(f\"   Training AUC: {train_auc:.4f}\")\n",
    "    print(f\"   Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = churn_model.stages[-1].featureImportances.toArray()\n",
    "feature_names = all_features\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîç Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Lifetime Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate historical CLV for training\n",
    "clv_data = customer_segments_labeled.withColumn(\n",
    "    \"historical_clv\",\n",
    "    col(\"total_spent\") * (lit(365) / greatest(col(\"customer_lifetime_days\"), lit(1)))\n",
    ").filter(col(\"customer_lifetime_days\") > 30)  # Filter customers with sufficient history\n",
    "\n",
    "# CLV prediction features\n",
    "clv_features = [\n",
    "    \"total_transactions\", \"avg_transaction_amount\", \"frequency_score\",\n",
    "    \"category_diversity\", \"weekend_preference\", \"age\"\n",
    "]\n",
    "\n",
    "# Prepare CLV dataset\n",
    "clv_prediction_data = clv_data.select(\n",
    "    [\"customer_id\", \"historical_clv\"] + clv_features + categorical_features\n",
    ").na.drop()\n",
    "\n",
    "print(f\"üí∞ CLV prediction dataset: {clv_prediction_data.count():,} customers\")\n",
    "print(f\"üìä Average historical CLV: ${clv_prediction_data.agg(avg('historical_clv')).collect()[0][0]:.2f}\")\n",
    "\n",
    "# Create CLV prediction pipeline\n",
    "clv_string_indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "clv_one_hot_encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_enc\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "clv_all_features = clv_features + [f\"{col}_enc\" for col in categorical_features]\n",
    "\n",
    "clv_assembler = VectorAssembler(\n",
    "    inputCols=clv_all_features,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "clv_scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Random Forest regressor for CLV prediction\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"historical_clv\",\n",
    "    predictionCol=\"predicted_clv\",\n",
    "    numTrees=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "clv_pipeline = Pipeline(stages=(\n",
    "    clv_string_indexers + clv_one_hot_encoders +\n",
    "    [clv_assembler, clv_scaler, rf_regressor]\n",
    "))\n",
    "\n",
    "# Split data for CLV prediction\n",
    "clv_train, clv_test = clv_prediction_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train CLV model with MLflow\n",
    "with mlflow.start_run(run_name=\"clv_prediction_rf\"):\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "    mlflow.log_param(\"num_trees\", 100)\n",
    "    mlflow.log_param(\"features\", \",\".join(clv_all_features))\n",
    "    \n",
    "    # Train model\n",
    "    clv_model = clv_pipeline.fit(clv_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    clv_train_predictions = clv_model.transform(clv_train)\n",
    "    clv_test_predictions = clv_model.transform(clv_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    clv_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"historical_clv\",\n",
    "        predictionCol=\"predicted_clv\",\n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "    \n",
    "    train_rmse = clv_evaluator.evaluate(clv_train_predictions)\n",
    "    test_rmse = clv_evaluator.evaluate(clv_test_predictions)\n",
    "    \n",
    "    # R-squared\n",
    "    clv_evaluator.setMetricName(\"r2\")\n",
    "    train_r2 = clv_evaluator.evaluate(clv_train_predictions)\n",
    "    test_r2 = clv_evaluator.evaluate(clv_test_predictions)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.spark.log_model(clv_model, \"clv_model\")\n",
    "    \n",
    "    print(f\"\\nüí∞ CLV Model Performance:\")\n",
    "    print(f\"   Training RMSE: ${train_rmse:.2f}\")\n",
    "    print(f\"   Test RMSE: ${test_rmse:.2f}\")\n",
    "    print(f\"   Training R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"   Test R¬≤: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Recommendation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import explode, collect_list\n",
    "\n",
    "# Prepare data for collaborative filtering\n",
    "# Create user-item ratings matrix from transaction data\n",
    "user_item_ratings = customer_transactions \\\n",
    "    .groupBy(\"customer_id\", \"product_id\") \\\n",
    "    .agg(\n",
    "        count(\"transaction_id\").alias(\"purchase_count\"),\n",
    "        sum(\"transaction_amount\").alias(\"total_spent_on_product\")\n",
    "    ) \\\n",
    "    .withColumn(\"rating\", \n",
    "               least(lit(5.0), \n",
    "                    (col(\"purchase_count\") * 2 + \n",
    "                     col(\"total_spent_on_product\") / 100) / 2))\n",
    "\n",
    "# Create integer IDs for ALS\n",
    "user_indexer = StringIndexer(inputCol=\"customer_id\", outputCol=\"user_id\")\n",
    "item_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"item_id\")\n",
    "\n",
    "user_indexed = user_indexer.fit(user_item_ratings).transform(user_item_ratings)\n",
    "user_item_indexed = item_indexer.fit(user_indexed).transform(user_indexed)\n",
    "\n",
    "print(f\"üõí Recommendation dataset: {user_item_indexed.count():,} user-item interactions\")\n",
    "print(f\"üë• Unique users: {user_item_indexed.select('user_id').distinct().count():,}\")\n",
    "print(f\"üì¶ Unique items: {user_item_indexed.select('item_id').distinct().count():,}\")\n",
    "\n",
    "# Split data\n",
    "rec_train, rec_test = user_item_indexed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ALS model for collaborative filtering\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train recommendation model\n",
    "with mlflow.start_run(run_name=\"product_recommendations_als\"):\n",
    "    mlflow.log_param(\"algorithm\", \"ALS\")\n",
    "    mlflow.log_param(\"max_iter\", 10)\n",
    "    mlflow.log_param(\"reg_param\", 0.1)\n",
    "    \n",
    "    rec_model = als.fit(rec_train)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    user_recs = rec_model.recommendForAllUsers(10)\n",
    "    item_recs = rec_model.recommendForAllItems(10)\n",
    "    \n",
    "    # Evaluate model\n",
    "    rec_predictions = rec_model.transform(rec_test)\n",
    "    rec_evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    rec_rmse = rec_evaluator.evaluate(rec_predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"recommendation_rmse\", rec_rmse)\n",
    "    mlflow.spark.log_model(rec_model, \"recommendation_model\")\n",
    "    \n",
    "    print(f\"\\nüéØ Recommendation Model Performance:\")\n",
    "    print(f\"   RMSE: {rec_rmse:.4f}\")\n",
    "\n",
    "# Example: Get recommendations for top customers\n",
    "top_customers = customer_segments_labeled \\\n",
    "    .filter(col(\"segment_name\") == \"Champions\") \\\n",
    "    .select(\"customer_id\") \\\n",
    "    .limit(5)\n",
    "\n",
    "print(\"\\nüåü Sample Recommendations for Champion Customers:\")\n",
    "sample_recommendations = user_recs.join(\n",
    "    user_indexer.fit(user_item_ratings).transform(top_customers),\n",
    "    \"user_id\"\n",
    ").select(\"customer_id\", \"recommendations\")\n",
    "\n",
    "sample_recommendations.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unified scoring pipeline\n",
    "def score_customers(customer_data, churn_model, clv_model, rec_model, segment_model):\n",
    "    \"\"\"\n",
    "    Score customers with all models for real-time insights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Customer segmentation\n",
    "    segmented_customers = segment_model.transform(customer_data)\n",
    "    \n",
    "    # Churn prediction\n",
    "    churn_predictions = churn_model.transform(segmented_customers)\n",
    "    \n",
    "    # CLV prediction\n",
    "    clv_predictions = clv_model.transform(churn_predictions)\n",
    "    \n",
    "    # Extract probabilities and create final scores\n",
    "    final_scores = clv_predictions \\\n",
    "        .withColumn(\"churn_probability\", \n",
    "                   col(\"probability\").getItem(1)) \\\n",
    "        .withColumn(\"customer_score\",\n",
    "                   (col(\"predicted_clv\") * (lit(1) - col(\"churn_probability\"))) / 1000) \\\n",
    "        .withColumn(\"priority_tier\",\n",
    "                   when(col(\"customer_score\") > 2, \"Tier 1 - Highest Priority\")\n",
    "                   .when(col(\"customer_score\") > 1, \"Tier 2 - High Priority\")\n",
    "                   .when(col(\"customer_score\") > 0.5, \"Tier 3 - Medium Priority\")\n",
    "                   .otherwise(\"Tier 4 - Low Priority\")) \\\n",
    "        .withColumn(\"scoring_timestamp\", current_timestamp())\n",
    "    \n",
    "    return final_scores\n",
    "\n",
    "# Score all customers\n",
    "customer_scores = score_customers(\n",
    "    customer_features,\n",
    "    churn_model,\n",
    "    clv_model,\n",
    "    rec_model,\n",
    "    clustering_model\n",
    ")\n",
    "\n",
    "print(\"üìä Customer Scoring Summary:\")\n",
    "customer_scores.groupBy(\"priority_tier\", \"segment_name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        avg(\"customer_score\").alias(\"avg_score\"),\n",
    "        avg(\"churn_probability\").alias(\"avg_churn_risk\"),\n",
    "        avg(\"predicted_clv\").alias(\"avg_predicted_clv\")\n",
    "    ) \\\n",
    "    .orderBy(\"priority_tier\", \"segment_name\") \\\n",
    "    .show()\n",
    "\n",
    "# Save scored customers to Azure Data Lake\n",
    "output_path = f\"{DATA_LAKE_PATH}/gold/customer_ml_scores\"\n",
    "\n",
    "customer_scores.select(\n",
    "    \"customer_id\", \"segment_name\", \"cluster\", \"churn_probability\", \n",
    "    \"predicted_clv\", \"customer_score\", \"priority_tier\", \"scoring_timestamp\"\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Customer scores saved to: {output_path}\")\n",
    "\n",
    "# Create actionable insights\n",
    "actionable_insights = customer_scores.select(\n",
    "    \"customer_id\",\n",
    "    \"segment_name\",\n",
    "    \"churn_probability\",\n",
    "    \"predicted_clv\",\n",
    "    \"priority_tier\",\n",
    "    when(col(\"churn_probability\") > 0.7, \"High Risk - Immediate Intervention Required\")\n",
    "    .when(col(\"churn_probability\") > 0.5, \"Medium Risk - Engagement Campaign\")\n",
    "    .when(col(\"predicted_clv\") > 2000, \"High Value - VIP Treatment\")\n",
    "    .when(col(\"segment_name\") == \"At Risk\", \"Retention Campaign\")\n",
    "    .otherwise(\"Standard Journey\").alias(\"recommended_action\")\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Actionable Customer Insights:\")\n",
    "actionable_insights.groupBy(\"recommended_action\") \\\n",
    "    .agg(count(\"*\").alias(\"customer_count\")) \\\n",
    "    .orderBy(desc(\"customer_count\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Customer Analytics & ML Pipeline Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Models Trained:\")\n",
    "print(\"   ‚Ä¢ Customer Segmentation (K-Means): 5 segments\")\n",
    "print(f\"   ‚Ä¢ Churn Prediction (Random Forest): {test_auc:.3f} AUC\")\n",
    "print(f\"   ‚Ä¢ CLV Prediction (Random Forest): {test_r2:.3f} R¬≤\")\n",
    "print(f\"   ‚Ä¢ Product Recommendations (ALS): {rec_rmse:.3f} RMSE\")\n",
    "\n",
    "print(\"\\nüìä Business Impact:\")\n",
    "total_customers = customer_scores.count()\n",
    "high_risk_customers = customer_scores.filter(col(\"churn_probability\") > 0.7).count()\n",
    "high_value_customers = customer_scores.filter(col(\"predicted_clv\") > 2000).count()\n",
    "\n",
    "print(f\"   ‚Ä¢ Total Customers Analyzed: {total_customers:,}\")\n",
    "print(f\"   ‚Ä¢ High Churn Risk Customers: {high_risk_customers:,} ({high_risk_customers/total_customers*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ High Value Customers: {high_value_customers:,} ({high_value_customers/total_customers*100:.1f}%)\")\n",
    "\n",
    "predicted_total_clv = customer_scores.agg(sum(\"predicted_clv\")).collect()[0][0]\n",
    "print(f\"   ‚Ä¢ Predicted Total CLV: ${predicted_total_clv:,.2f}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   ‚Ä¢ Deploy models to Azure ML for real-time scoring\")\n",
    "print(\"   ‚Ä¢ Set up automated retraining pipelines\")\n",
    "print(\"   ‚Ä¢ Create Power BI dashboards for business users\")\n",
    "print(\"   ‚Ä¢ Implement A/B testing for marketing campaigns\")\n",
    "print(\"   ‚Ä¢ Set up real-time alerts for high-risk customers\")\n",
    "\n",
    "print(\"\\nüìà Model Artifacts Saved:\")\n",
    "print(f\"   ‚Ä¢ Customer scores: {output_path}\")\n",
    "print(\"   ‚Ä¢ MLflow models: Available in workspace\")\n",
    "print(\"   ‚Ä¢ Feature importance: Logged in MLflow\")\n",
    "\n",
    "print(\"\\nüîÑ Recommended Refresh Schedule:\")\n",
    "print(\"   ‚Ä¢ Customer scores: Daily\")\n",
    "print(\"   ‚Ä¢ Segmentation model: Weekly\")\n",
    "print(\"   ‚Ä¢ Churn/CLV models: Monthly\")\n",
    "print(\"   ‚Ä¢ Recommendation model: Weekly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}