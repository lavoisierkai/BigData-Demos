# Development Environment Configuration
# ===================================

environment:
  name: development
  region: us-west-2
  cost_center: engineering
  auto_shutdown: true

project:
  name: dataplatform
  owner: data-engineering-team
  version: "1.0.0"

# Common tags applied to all resources
common_tags:
  Department: "Data Engineering"
  CostCenter: "Engineering"
  Environment: "Development"
  Backup: "false"

# Terraform backend configuration
terraform:
  state_bucket: "terraform-state-dataplatform-dev"
  dynamodb_table: "terraform-locks-dataplatform-dev"

# Network configuration
network:
  create_vpc: true
  vpc_cidr: "10.0.0.0/16"
  availability_zones: 2
  enable_nat_gateway: false

# Data lake layers configuration
data_layers:
  - name: raw
    description: "Raw data ingestion layer"
    format: "json"
    versioning: "Enabled"
    lifecycle_enabled: true
    transitions:
      - days: 30
        storage_class: "STANDARD_IA"
      - days: 90
        storage_class: "GLACIER"
    expiration_days: 365
    abort_incomplete_multipart_upload_days: 7
    notification_enabled: false
    
  - name: processed
    description: "Cleaned and processed data layer"
    format: "parquet"
    versioning: "Enabled"
    lifecycle_enabled: true
    transitions:
      - days: 90
        storage_class: "STANDARD_IA"
      - days: 180
        storage_class: "GLACIER"
    expiration_days: 730
    abort_incomplete_multipart_upload_days: 7
    notification_enabled: true
    sqs_notifications:
      - queue_arn: "arn:aws:sqs:us-west-2:123456789012:data-processing-queue"
        events: ["s3:ObjectCreated:*"]
        filter_prefix: "year="
        filter_suffix: ".parquet"
    
  - name: curated
    description: "Business-ready curated data layer"
    format: "parquet"
    versioning: "Enabled"
    lifecycle_enabled: true
    transitions:
      - days: 180
        storage_class: "STANDARD_IA"
      - days: 365
        storage_class: "GLACIER"
    abort_incomplete_multipart_upload_days: 7
    notification_enabled: false

# Security configuration
security:
  encryption:
    enabled: true
    algorithm: "AES256"
    bucket_key_enabled: true
    # kms_key_id: "arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012"

# AWS Glue configuration
glue:
  catalog_encryption:
    enabled: false
    mode: "SSE-S3"
    # sse_aws_kms_key_id: "alias/aws/glue"
    # connection_kms_key_id: "alias/aws/glue"
    return_encrypted_password: true

# EMR configuration
emr:
  enabled: true
  use_existing_cluster: false
  release_label: "emr-6.9.0"
  applications: ["Spark", "Hadoop", "Hive", "Zeppelin"]
  key_name: "dataplatform-dev-key"
  
  # Instance configuration (smaller for dev)
  master_instance_type: "m5.xlarge"
  core_instance_type: "m5.large"
  core_instance_count: 2
  
  # EBS configuration
  master_ebs_config:
    size: 100
    type: "gp3"
    volumes_per_instance: 1
    
  core_ebs_config:
    size: 100
    type: "gp3"
    volumes_per_instance: 1
  
  # Auto scaling (disabled for dev)
  auto_scaling:
    enabled: false
    min_capacity: 1
    max_capacity: 3
    scale_out_adjustment: 1
    scale_out_cooldown: 300
    evaluation_periods: 1
    period: 300
    memory_threshold: 15
  
  # Security groups
  ssh_cidr_blocks: ["10.0.0.0/8"]  # VPC only
  web_ui_access: true
  web_ui_cidr_blocks: ["10.0.0.0/8"]
  
  # Optional: Task instance groups for spot instances
  task_instance_groups: []
  
  # Bootstrap actions
  bootstrap_actions:
    - name: "Install Additional Packages"
      path: "s3://dataplatform-dev-scripts/bootstrap/install-packages.sh"
      args: ["python3-dev", "gcc"]
  
  # Steps to run on cluster startup
  steps: []
  
  # Spark configuration
  configurations:
    - classification: "spark-defaults"
      properties:
        "spark.sql.adaptive.enabled": "true"
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
        "spark.sql.adaptive.skewJoin.enabled": "true"
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
        "spark.sql.hive.metastore.version": "3.1.0"
        "spark.sql.hive.metastore.jars": "builtin"
        "spark.dynamicAllocation.enabled": "true"
        "spark.dynamicAllocation.minExecutors": "1"
        "spark.dynamicAllocation.maxExecutors": "8"
    
    - classification: "spark-hive-site"
      properties:
        "javax.jdo.option.ConnectionURL": "jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true"
        "javax.jdo.option.ConnectionDriverName": "com.mysql.jdbc.Driver"
        "javax.jdo.option.ConnectionUserName": "hive"
        "javax.jdo.option.ConnectionPassword": "hive"
  
  idle_timeout: 3600  # 1 hour

# Compute resource sizing
compute:
  instance_types:
    small: "t3.medium"
    medium: "m5.large"
    large: "m5.xlarge"
  
  auto_scaling:
    min_instances: 1
    max_instances: 5
    target_cpu: 70

# Storage configuration
storage:
  replication: false
  backup_retention_days: 7
  lifecycle_enabled: true
  cross_region_backup: false

# Data pipeline settings
pipelines:
  schedule: "0 2 * * *"  # Daily at 2 AM
  retry_attempts: 3
  timeout_minutes: 60
  
# Monitoring configuration
monitoring:
  metrics_retention_days: 30
  log_level: "DEBUG"
  alerts_enabled: true
  detailed_monitoring: false

# Cost optimization settings for development
cost_optimization:
  use_spot_instances: true
  scheduled_shutdown: true
  shutdown_schedule: "0 22 * * MON-FRI"  # 10 PM weekdays
  startup_schedule: "0 8 * * MON-FRI"    # 8 AM weekdays